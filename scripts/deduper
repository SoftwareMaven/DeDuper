#!/usr/bin/env python3
import sys

import deduper

def process_args():
    from optparse import OptionParser
    usage = 'usage: %prog [options] path1 [path2...]'
    parser = OptionParser(usage=usage)
    parser.add_option("-r", "--report", dest="report", action="store_true", default=True,
                      help="Print report of known and suspected duplicates [Default]",)
    parser.add_option("-q", "--quiet", dest="report", action="store_false",
                      help="Shhh, be vewy, vewy quiet. (not recommended)",)
    parser.add_option("-i", "--ignore", dest="ignore", action="store", metavar='IGNORE_FILE',
                      help="List of regular expressions of files to ignore", default=None)
    parser.add_option("-d", "--delete", action="store_true", dest="delete", default=False,
                      help="Delete duplicates",)
    parser.add_option("-s", "--suspected", action="store_true",
                      default=False, dest="suspected",
                      help="Try to find duplicates using more than the file's bits",)

    (options, paths) = parser.parse_args()

    if len(paths) == 0:
        print('At least one directory to look in is required', file=sys.stderr)
        exit(2)
    else:
        return options, paths

options, paths = process_args()
ignore_list = []
if options.ignore:
    with open(options.ignore, 'r') as f:
        ignore_list = list(f.readlines())

if options.report:
    path_count = 0
    def progress_func(path, dups_found):
        global path_count
        path_count += 1
        if path_count % 5 == 0:
            print('Processing: {path} [{dups_found} duplicates found]'.format(**locals()))
    progress = progress_func
else:
    progress = None

context = find_duplicates(paths, include_suspected=options.suspected,
                          ignore_re_list=ignore_list, progress=progress)
